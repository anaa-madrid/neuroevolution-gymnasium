{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f85c3d3",
   "metadata": {},
   "source": [
    "# Neuroevolution with Gymnasium\n",
    "## Lunar Lander experiments\n",
    "\n",
    "Name: Ana Madrid Serrano"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0fd142",
   "metadata": {},
   "source": [
    "This notebook explores different ways of interacting with the **LunarLander-v2** environment from the Gymnasium library.\n",
    "\n",
    "The experiments are structured progressively:\n",
    "\n",
    "1. **Manual control** of the environment by a human user, to understand the task.\n",
    "2. **A rule-based agent**, using a handcrafted policy based on observations.\n",
    "3. **A neural model**, designed as a basis for future neuroevolutionary approaches.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ee95ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[box2d] in c:\\users\\anadr\\anaconda3\\lib\\site-packages (0.27.1)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: jax-jumpy>=0.2.0 in c:\\users\\anadr\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (1.0.0)\n",
      "\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\anadr\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (1.22.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\anadr\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (2.0.0)\n",
      "Requirement already satisfied: gymnasium-notices>=0.0.1 in c:\\users\\anadr\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (0.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\anadr\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (4.8.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\anadr\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (4.10.0)\n",
      "Requirement already satisfied: pygame==2.1.3.dev8 in c:\\users\\anadr\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (2.1.3.dev8)\n",
      "Requirement already satisfied: swig==4.* in c:\\users\\anadr\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (4.1.1)\n",
      "Requirement already satisfied: box2d-py==2.3.5 in c:\\users\\anadr\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (2.3.5)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\anadr\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.8.0->gymnasium[box2d]) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "pip install gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f50695d",
   "metadata": {},
   "source": [
    "## 1. Human control of Lunar Lander\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dbda0e",
   "metadata": {},
   "source": [
    "Before implementing any automated agent, the environment is tested using **manual control**.\n",
    "This allows direct observation of the state variables, the effect of actions, and the difficulty of the landing task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b40957f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anadr\\Anaconda3\\lib\\site-packages\\ipykernel\\eventloops.py:256: RuntimeWarning: coroutine 'Kernel.do_one_iteration' was never awaited\n",
      "  self.func()\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")\n",
    "\n",
    "import numpy as np\n",
    "import pygame\n",
    "import gymnasium.utils.play\n",
    "\n",
    "lunar_lander_keys = {\n",
    "    (pygame.K_UP,): 2,\n",
    "    (pygame.K_LEFT,): 1,\n",
    "    (pygame.K_RIGHT,): 3,\n",
    "}\n",
    "gymnasium.utils.play.play(env, zoom=3, keys_to_action=lunar_lander_keys, noop=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a9af06",
   "metadata": {},
   "source": [
    "## 2. Rule-based agent (heuristic policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7241d2a",
   "metadata": {},
   "source": [
    "The Lunar Lander environment provides an **observation vector** with the following components:\n",
    "\n",
    "observation = [x, y, vx, vy, angle, angular_velocity, left_leg, right_leg]\n",
    "\n",
    "The **action space** is discrete and defined as:\n",
    "\n",
    "actions = [do nothing, fire left engine, fire main engine, fire right engine]\n",
    "\n",
    "\n",
    "\n",
    "Based on these variables, a simple handcrafted policy is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cbb07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy (observation):\n",
    "    if observation[3]<-0.2:\n",
    "        print('⬆︎',end='')\n",
    "        return 2\n",
    "    \n",
    "    if observation[4]<-0.1:\n",
    "        print('⬅︎',end='')\n",
    "        return 1 \n",
    "        \n",
    "    if observation[5]<-0.1:\n",
    "        print('⬅︎',end='')\n",
    "        return 1\n",
    "    \n",
    "    if observation[2]<-0.1:\n",
    "        print('➡︎',end='')\n",
    "        return 3\n",
    "    \n",
    "    if observation[0]<-0.1:\n",
    "        print('➡︎',end='')\n",
    "        return 3\n",
    " \n",
    "    if observation[4]> 0.1:\n",
    "        print('➡︎',end='')\n",
    "        return 3\n",
    "\n",
    "    if observation[5]>0.1:\n",
    "        print('➡︎',end='')\n",
    "        return 3\n",
    "\n",
    "    if observation[2]>0.1:\n",
    "        print('⬅︎',end='')\n",
    "        return 1\n",
    "    \n",
    "    if observation[0]>0.1:\n",
    "        print('⬅︎',end='')\n",
    "        return 1\n",
    "    \n",
    "    if observation[3]>0.1:\n",
    "        print('⬇︎',end='')\n",
    "        return 0\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f27464",
   "metadata": {},
   "source": [
    "The environment is now executed using the heuristic policy.\n",
    "At each time step, the policy receives the current observation and selects an action.\n",
    "\n",
    "The accumulated reward is used as a performance measure for the episode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d8a866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡︎➡︎➡︎➡︎➡︎➡︎⬅︎➡︎⬅︎➡︎⬅︎➡︎⬅︎➡︎⬅︎➡︎⬅︎➡︎⬅︎➡︎⬅︎➡︎⬅︎➡︎⬅︎⬆︎⬅︎⬆︎⬆︎⬅︎⬆︎⬅︎⬆︎⬅︎⬆︎⬅︎⬆︎⬅︎⬆︎➡︎⬆︎➡︎⬆︎⬆︎⬆︎➡︎⬆︎➡︎⬆︎⬆︎⬆︎➡︎➡︎⬆︎⬆︎➡︎⬆︎⬅︎⬆︎➡︎⬆︎⬅︎⬆︎⬅︎⬆︎⬅︎⬆︎⬅︎⬆︎⬅︎⬆︎⬅︎⬆︎⬆︎⬅︎⬆︎⬆︎➡︎⬆︎⬆︎➡︎➡︎⬆︎➡︎⬆︎⬆︎➡︎⬆︎⬅︎⬆︎⬆︎⬅︎⬆︎⬅︎⬆︎⬆︎⬅︎⬆︎⬆︎➡︎⬆︎➡︎⬆︎⬆︎➡︎⬆︎➡︎➡︎⬆︎➡︎⬆︎⬆︎⬅︎⬆︎⬅︎⬆︎⬅︎⬆︎⬅︎⬆︎⬅︎⬅︎⬆︎⬅︎⬆︎⬅︎⬆︎➡︎⬆︎⬆︎⬆︎⬆︎➡︎⬆︎➡︎⬆︎⬆︎➡︎⬆︎⬆︎➡︎⬆︎⬅︎➡︎⬆︎⬆︎⬆︎➡︎⬆︎⬅︎⬆︎⬆︎⬅︎⬆︎⬅︎⬆︎⬅︎⬆︎⬅︎⬆︎⬅︎⬆︎⬆︎⬅︎⬆︎⬅︎⬆︎⬆︎➡︎⬆︎➡︎⬆︎➡︎⬆︎➡︎⬆︎➡︎⬆︎➡︎⬆︎➡︎⬆︎⬆︎⬆︎⬅︎⬅︎⬆︎⬅︎⬆︎⬆︎⬆︎⬆︎⬅︎⬆︎⬆︎⬅︎⬆︎⬆︎⬅︎⬆︎⬅︎⬆︎⬅︎⬆︎⬆︎⬅︎⬆︎➡︎⬆︎⬆︎➡︎⬆︎➡︎⬆︎➡︎⬆︎➡︎➡︎⬆︎➡︎⬆︎⬆︎⬅︎⬆︎⬆︎⬅︎⬆︎⬅︎⬆︎⬅︎⬆︎⬆︎⬆︎⬆︎⬅︎⬆︎⬆︎⬅︎⬅︎⬆︎⬆︎⬆︎➡︎⬆︎➡︎⬆︎➡︎⬆︎➡︎⬆︎⬆︎⬆︎➡︎⬆︎⬆︎➡︎⬅︎⬆︎⬅︎⬆︎⬅︎⬆︎⬆︎⬅︎⬆︎➡︎⬆︎➡︎⬆︎⬆︎⬆︎➡︎⬆︎⬆︎➡︎⬆︎⬅︎⬆︎⬅︎⬆︎⬆︎⬅︎⬆︎⬅︎⬆︎⬅︎⬆︎⬆︎➡︎⬆︎⬆︎➡︎⬆︎⬆︎➡︎⬆︎➡︎⬆︎➡︎⬆︎➡︎⬆︎⬆︎⬅︎➡︎⬆︎⬅︎⬆︎⬆︎⬆︎⬆︎⬅︎⬆︎⬆︎⬅︎⬆︎⬅︎⬆︎⬅︎⬆︎⬆︎⬅︎⬆︎⬅︎⬅︎⬆︎⬅︎⬆︎➡︎⬆︎➡︎⬆︎⬆︎➡︎⬆︎➡︎⬆︎➡︎⬆︎⬅︎⬆︎⬅︎⬆︎⬅︎⬅︎⬆︎⬆︎➡︎⬆︎⬅︎➡︎➡︎➡︎➡︎➡︎➡︎➡︎➡︎⬅︎➡︎➡︎➡︎⬅︎⬅︎⬅︎⬅︎⬅︎⬅︎⬅︎⬅︎⬅︎⬅︎⬅︎⬅︎⬅︎⬅︎⬅︎⬅︎⬅︎⬅︎⬅︎⬅︎⬅︎⬅︎⬅︎⬅︎260.5318062615744 0.6302659031307871\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "260.5318062615744"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "\n",
    "def run ():\n",
    "    observation, info = env.reset()\n",
    "    racum = 0\n",
    "\n",
    "    while True:\n",
    "        action = policy(observation)\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        racum += reward\n",
    "\n",
    "        if terminated or truncated:\n",
    "            r = (racum + 1000) / 2000\n",
    "            print(racum, r)\n",
    "            return racum\n",
    "    \n",
    "run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1a55ef",
   "metadata": {},
   "source": [
    "## 3. Neural model for neuroevolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f436e3b8",
   "metadata": {},
   "source": [
    "In this section, a **neuroevolutionary approach** is applied to the Lunar Lander problem.\n",
    "\n",
    "Instead of designing rules manually or training neural networks using gradient-based methods,\n",
    "the agent's behavior is optimized using **SALGA (Simple Adaptive Learning Genetic Algorithm)**.\n",
    "\n",
    "SALGA evolves a population of candidate solutions, called chromosomes, by applying genetic operators\n",
    "such as selection, mutation, and recombination.\n",
    "Each chromosome encodes the parameters of a neural controller.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6541ed",
   "metadata": {},
   "source": [
    "The agent is represented by a **single-layer perceptron** that maps environment observations to actions.\n",
    "\n",
    "- **Inputs (8):** Lunar Lander observation vector  \n",
    "- **Outputs (4):** Discrete action space  \n",
    "\n",
    "The perceptron parameters (weights and biases) are encoded as a chromosome,\n",
    "making them suitable for evolutionary optimization using SALGA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0e9a6e",
   "metadata": {},
   "source": [
    "A perceptron model is instantiated to serve as the neural controller of the agent.\n",
    "The structure of the network remains fixed during evolution; only its parameters are evolved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a34dfffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Perceptron:\n",
    "    def __init__(self, ninput, noutput):\n",
    "        self.ninput = ninput\n",
    "        self.noutput = noutput\n",
    "        self.w = np.random.rand(ninput,noutput)-0.5\n",
    "        self.b = np.random.rand(noutput)-0.5\n",
    "        \n",
    "    def forward (self, x): # propaga un vector x y devuelve la salida\n",
    "        u = np.dot(x, self.w) + self.b              \n",
    "        return np.piecewise(u, [u<0, u>=0], [0,1])\n",
    "                   \n",
    "        \n",
    "    def update (self, x, d, alpha): # realiza una iteración de entrenamiento\n",
    "        s = self.forward(x) # propaga\n",
    "        # Calcula la actualización de los pesos y el sesgo\n",
    "        error = d - s\n",
    "        self.w += alpha * np.outer(x,error)\n",
    "        self.b += error*alpha\n",
    "        \n",
    "               \n",
    "    def RMS (self, X, D): # calcula el error RMS\n",
    "        S = self.forward(X)\n",
    "        return np.mean(np.sqrt(np.mean(np.square(S-D),axis=1)))\n",
    "        \n",
    "    def accuracy (self, X, D): # calcula el ratio de aciertos\n",
    "        S = self.forward(X)\n",
    "        errors = np.mean(np.abs(D-S))\n",
    "        return 1.0 - errors\n",
    "    \n",
    "    def info (self, X, D): # traza de cómno va el entrenamiento\n",
    "        print('     RMS: %6.5f' % self.RMS(X,D))\n",
    "        print('Accuracy: %6.5f' % self.accuracy(X,D))\n",
    "        \n",
    "    def train (self, X, D, alpha, epochs, trace=0): # entrena usando update\n",
    "        for e in range(1,epochs+1):\n",
    "            for i in range(len(X)):\n",
    "                self.update(X[i],D[i], alpha)\n",
    "            if trace!=0 and e%trace == 0:\n",
    "                print('\\n   Epoch: %d' % e)\n",
    "                self.info(X,D)\n",
    "\n",
    "    def from_chromosome(self, chromosome):\n",
    "    # Extraer los pesos y bias de la lista del cromosoma\n",
    "        w_size = self.ninput * self.noutput\n",
    "        w = np.array(chromosome[:w_size]).reshape(self.ninput, self.noutput)\n",
    "        b = np.array(chromosome[w_size:w_size+self.noutput])\n",
    "\n",
    "        # Actualizar los pesos y bias de la red\n",
    "        self.w = w\n",
    "        self.b = b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e6e154",
   "metadata": {},
   "source": [
    "The following chromosome corresponds to the **best individual obtained after applying SALGA**.\n",
    "\n",
    "This chromosome was selected based on its fitness, defined as the accumulated reward obtained\n",
    "by the agent during an episode of the Lunar Lander environment.\n",
    "\n",
    "\n",
    "ch=[-0.061264477472171563, 1.2334191493374835, 0.40603380952043866, 0.9553591142153932, 0.14556935003108212, 0.17990814841861139, -0.8292220744876733, 0.3686721742648328, 0.1958597704484823, 2.3265527364414105, 1.1800661108170107, 1.1311726379626674, 2.4006362522310893, 0.754321877904867, -2.00412648105665, -0.22351230687257362, -0.24490131279372235, -2.5545128379705715, -1.7445478055667343, -1.1100652947300667, 1.9377778639443637, -1.0145382491395563, -2.1039384504087746, 0.5057075365908111, 0.47659236811333505, -0.7576359187542151, -0.3814027318230223, 1.1234334031505608, 1.425999767768399, -0.6016325392277657, 1.2577655010588522, -1.4565977437520088, -0.1549568560654263, 0.025309412884416155, -0.13978695682664993, 2.0723721822560996]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7fcafd",
   "metadata": {},
   "source": [
    "The chromosome is decoded and loaded into the perceptron.\n",
    "This process assigns the evolved weights and biases to the neural network,\n",
    "fully defining the behavior of the agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872b9955",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Perceptron(8,4)\n",
    "ch=[-0.061264477472171563, 1.2334191493374835, 0.40603380952043866, 0.9553591142153932, 0.14556935003108212, 0.17990814841861139, -0.8292220744876733, 0.3686721742648328, 0.1958597704484823, 2.3265527364414105, 1.1800661108170107, 1.1311726379626674, 2.4006362522310893, 0.754321877904867, -2.00412648105665, -0.22351230687257362, -0.24490131279372235, -2.5545128379705715, -1.7445478055667343, -1.1100652947300667, 1.9377778639443637, -1.0145382491395563, -2.1039384504087746, 0.5057075365908111, 0.47659236811333505, -0.7576359187542151, -0.3814027318230223, 1.1234334031505608, 1.425999767768399, -0.6016325392277657, 1.2577655010588522, -1.4565977437520088, -0.1549568560654263, 0.025309412884416155, -0.13978695682664993, 2.0723721822560996]\n",
    "model.from_chromosome(ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d66d40",
   "metadata": {},
   "source": [
    "A policy function is defined using the evolved neural controller.\n",
    "\n",
    "For each observation:\n",
    "1. The perceptron computes the output activations\n",
    "2. The action corresponding to the maximum activation is selected\n",
    "\n",
    "This policy directly reflects the solution found by the SALGA algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef66e2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy (observation):\n",
    "    s = model.forward(observation)\n",
    "    action = np.argmax(s)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218f88cd",
   "metadata": {},
   "source": [
    "This section evaluates an agent in the **LunarLander-v2** environment using the previously defined policy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a50806",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'policy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12784/139995065.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mracum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12784/139995065.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mracum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'policy' is not defined"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "\n",
    "def run ():\n",
    "    observation, info = env.reset()\n",
    "    ite = 0\n",
    "    racum = 0\n",
    "    while True:\n",
    "        action = policy(observation)\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        racum += reward\n",
    "\n",
    "        if terminated or truncated:\n",
    "            r = (racum+1000) / 2000\n",
    "            print(racum, r)\n",
    "            return racum\n",
    "    \n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc09f3d",
   "metadata": {},
   "source": [
    "The obtained reward provides an indication of the agent's performance.\n",
    "\n",
    "Higher accumulated rewards generally correspond to smoother landings,\n",
    "lower fuel consumption, and successful touchdowns."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
